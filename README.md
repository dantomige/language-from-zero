# ğŸ§  Language-From-Zero  
### A System-Aware LLM Built from Scratch in PyTorch

**Language-From-Zero** is a full-stack implementation of a **decoder-only Transformer (GPT-style) LLM** built entirely from scratch in PyTorch.

The focus is on understanding and engineering the full pipeline:

> raw data â†’ tokenizer â†’ datasets â†’ transformer â†’ trainer â†’ inference â†’ interactive assistant

---

## ğŸ§± Architecture

**Decoder-only Transformer (GPT-style)**  
- Causal self-attention  
- Multi-head attention  
- Token + positional embeddings  
- Residual connections + LayerNorm  
- Autoregressive decoding  

---

### ğŸ“š Training Data
- **Project Gutenberg** (long-form text)  
- **ShareGPT** (conversational data)  

### ğŸ— Training System
- Validation sets  
- L2 Regularization and label smoothing  
- Checkpointing  

### ğŸ§¬ System Awareness
- Context injection for:
  - DB schemas  
  - APIs  
  - Latency budgets  
  - Infrastructure context  

### ğŸ’¬ Inference
- Autoregressive decoding  
- Temperature
- Gradio Interface

---

## ğŸ Quick Start

### Training & Running the Assistant

```bash
python train.py
```

Run training to generate a new experiment folder

Copy the folder name that gets created

Paste it into app.py as:

experiments_folder_name = "your_experiment_folder_name_here"

```bash
python app.py
```